{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Demo_Vid.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "rET4bXRp8mm6",
        "L43TF_uc87We",
        "kiMnsuVc9OSZ",
        "D1PLjoEK7y0u"
      ],
      "machine_shape": "hm",
      "private_outputs": true,
      "authorship_tag": "ABX9TyPnrxCshXGOpoBP8r42LdC7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Victoooooor/ErgoChairML/blob/main/Demo_Vid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Environment"
      ],
      "metadata": {
        "id": "rET4bXRp8mm6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "LamhSqRnZTBY"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "!apt-get install libmagic-dev\n",
        "!pip install git+https://github.com/Victoooooor/ErgoChairML.git\n",
        "!pip install -q imageio\n",
        "!pip install tensorflow-io\n",
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-`lsb_release -c -s` main\" | sudo tee /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
        "!sudo apt-get -y -q update\n",
        "!sudo apt-get -y -q install gcsfuse"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOGIN"
      ],
      "metadata": {
        "id": "RCusvyFrlmDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "JH6l8bL_lkLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load trained model"
      ],
      "metadata": {
        "id": "L43TF_uc87We"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bucket_name = 'ergo_chair_ml'\n",
        "!gsutil -m cp -r gs://{bucket_name}/* ./"
      ],
      "metadata": {
        "id": "5r70Nv83cpHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Init"
      ],
      "metadata": {
        "id": "kiMnsuVc9OSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gen_chair import pix2pix\n",
        "\n",
        "from gen_chair import coco\n",
        "from gen_chair.gen_multi import Preprocess\n",
        "from gen_chair import pix2pix\n",
        "\n",
        "from google.colab import files\n",
        "import shutil\n",
        "import os\n",
        "import tqdm\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from PIL import ImageSequence\n",
        "import PIL\n",
        "import numpy as np\n",
        "import tensorflow_io as tfio \n",
        "\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "\n",
        "import io\n",
        "import html\n",
        "import time\n",
        "\n",
        "class InferenceConfig(coco.CocoConfig):\n",
        "  # Set batch size to 1 since we'll be running inference on\n",
        "  # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n",
        "  GPU_COUNT = 1\n",
        "  IMAGES_PER_GPU = 1\n",
        "\n",
        "cpdir_ske = 'skeleton_checkpoints'\n",
        "cpdir_mask = 'masked_checkpoints'\n",
        "\n",
        "class colab_ref(object):\n",
        "  def __init__(self):\n",
        "\n",
        "    self.prep = Preprocess(InferenceConfig)\n",
        "\n",
        "    \n",
        "    self.masked = pix2pix(cpdir_mask)\n",
        "    self.masked.loadcp()\n",
        "\n",
        "    \n",
        "    self.skeleton = pix2pix(cpdir_ske)\n",
        "    self.skeleton.loadcp()\n",
        "\n",
        "  def Infer(self, preproc, generator, origin, masked = True):\n",
        "\n",
        "    pre = preproc(origin)\n",
        "    if type(pre) is tuple:\n",
        "        if masked:\n",
        "          seg = pre[0]\n",
        "        else:\n",
        "          seg = pre[1]\n",
        "    else:\n",
        "        seg = None\n",
        "    origin = tf.cast(tf.image.resize_with_pad(origin, 256, 256), dtype=tf.int32)\n",
        "\n",
        "    if seg is not None:\n",
        "      seg  = tf.keras.preprocessing.image.img_to_array(seg)\n",
        "\n",
        "    else:\n",
        "      seg = origin\n",
        "\n",
        "    seg = tf.expand_dims(seg, axis=0)\n",
        "    seg = tf.cast(tf.image.resize_with_pad(seg, 256, 256), dtype=tf.int32)\n",
        "\n",
        "    gen = generator(seg ,training=True)\n",
        "    gen = tf.keras.utils.array_to_img(gen[0])\n",
        "\n",
        "    seg = tf.keras.utils.array_to_img(seg[0])\n",
        "\n",
        "    origin = tf.keras.utils.array_to_img(origin)\n",
        "    dst = PIL.Image.new('RGB', (origin.width + seg.width + gen.width, origin.height))\n",
        "    dst.paste(origin, (0, 0))\n",
        "    dst.paste(seg, (origin.width, 0))\n",
        "    dst.paste(gen, (origin.width+seg.width, 0))\n",
        "    return dst\n",
        "    \n",
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8, wid = 768, hei = 512):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: { width: {ideal: wid}, height: {ideal: hei}, facingMode: 'user'}});\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "\n",
        "  # get photo data\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  # get OpenCV format image\n",
        "  img = js_to_image(data) \n",
        "  cv2.imwrite(filename, img)\n",
        "  return filename\n",
        "\n",
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 768, 512);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = 774;\n",
        "      // div.style.maxWidth = '774px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = 768;\n",
        "      video.height = 512;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      // imgElement.style.position = 'absolute';\n",
        "      // imgElement.style.zIndex = 1;\n",
        "      imgElement.width = 768;\n",
        "      imgElement.height = 512;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 768; //video.videoWidth;\n",
        "      captureCanvas.height = 512; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        // imgElement.style.top = videoRect.top + \"px\";\n",
        "        // imgElement.style.left = videoRect.left + \"px\";\n",
        "        // imgElement.style.width = videoRect.width + \"px\";\n",
        "        // imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  \n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ],
      "metadata": {
        "id": "YUiUuoMabuPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Video Input"
      ],
      "metadata": {
        "id": "EbKw4Q7s4VFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "cl = colab_ref()\n",
        "# start streaming video from webcam\n",
        "video_stream()\n",
        "# label for video\n",
        "label_html = 'Capturing...'\n",
        "# initialze bounding box to empty\n",
        "bbox = ''\n",
        "count = 0 \n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    # convert JS response to OpenCV Image\n",
        "    img = js_to_image(js_reply[\"img\"])\n",
        "    img = tf.convert_to_tensor(img)\n",
        "    img= tfio.experimental.color.bgr_to_rgb(img)\n",
        "    mframe = cl.Infer(cl.prep.img_seg, cl.masked.generator, img, True)\n",
        "    sframe = cl.Infer(cl.prep.img_seg, cl.skeleton.generator, img, False)\n",
        "    # create transparent overlay for bounding box\n",
        "    bbox_array = np.full([512, 768, 4], 255, dtype=np.uint8)\n",
        "    bbox_array[:256, :,:3] = mframe\n",
        "    bbox_array[256:, : , :3] = sframe\n",
        "\n",
        "    # convert overlay of bbox into bytes\n",
        "    bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "    # update bbox so next frame gets new overlay\n",
        "    bbox = bbox_bytes"
      ],
      "metadata": {
        "id": "xhR4fIhZ3sqV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}